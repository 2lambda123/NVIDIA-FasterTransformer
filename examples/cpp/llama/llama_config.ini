[ft_instance_hyperparameter]
model_name=llama_33B
model_dir=../models/llama
data_type=fp16
pipeline_para_size=4


[request]
request_batch_size=32
start_pos=2

[llama_33B]
head_num=52
size_per_head=128
vocab_size=32000
decoder_layers=60
rotary_embedding=128
multiple_of=256
max_seq_len=1024
padding_id=0
random_seed=0
