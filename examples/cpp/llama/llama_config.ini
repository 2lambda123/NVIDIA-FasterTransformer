[ft_instance_hyperparameter]
model_name=llama_33B
model_dir=../models/llama
data_type=fp16
pipeline_para_size=4


[request]
beam_width=1 # beam width for beam search
request_batch_size=4 # determine by the request

[llama_33B]
head_num=52
size_per_head=128
vocab_size=32000
decoder_layers=60
rotary_embedding=128
multiple_of=256
padding_id=0
