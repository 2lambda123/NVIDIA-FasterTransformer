[ft_instance_hyperparameter]
model_name=llama_33B
model_dir=../models/llam
data_type=fp16
pipeline_para_size=4


[request]
beam_width=1 # beam width for beam search
request_batch_size=8 # determine by the request
request_output_len=0 # determine by the request

[llama_33B]
head_num=52
size_per_head=128
vocab_size=32000
decoder_layers=60
rotary_embedding=128
multiple_of=256
start_id=0
end_id=2

use_gptj_residual=1
